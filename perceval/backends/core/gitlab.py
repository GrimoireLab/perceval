# -*- coding: utf-8 -*-
#
# Copyright (C) 2015-2017 Bitergia
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, 51 Franklin Street, Fifth Floor, Boston, MA 02110-1335, USA.
#
# Authors:
#     ...
#

import json
import logging
import time

import requests
import urllib.parse

from grimoirelab.toolkit.datetime import datetime_to_utc, str_to_datetime
from grimoirelab.toolkit.uris import urijoin

from ...backend import (Backend,
                        BackendCommand,
                        BackendCommandArgumentParser,
                        metadata)
from ...errors import CacheError, RateLimitError
from ...utils import DEFAULT_DATETIME

GITLAB_URL = "https://gitlab.com/"
GITLAB_API_URL = "https://gitlab.com/api/v4"

# Range before sleeping until rate limit reset
MIN_RATE_LIMIT = 10
MAX_RATE_LIMIT = 500

TARGET_ISSUE_FIELDS = ['user_notes_count', 'award_emoji']

logger = logging.getLogger(__name__)


class GitLab(Backend):
    """GitLab backend for Perceval.

    This class allows the fetch the issues stored in GitLab
    repository.

    :param owner: GitLab owner
    :param repository: GitLab repository from the owner
    :param api_token: GitLab auth token to access the API
    :param tag: label used to mark the data
    :param base_url: GitLab URL in enterprise edition case;
        when no value is set the backend will be fetch the data
        from the GitLab public site.
    :param cache: use issues already retrieved in cache
    :param sleep_for_rate: sleep until rate limit is reset
    :param min_rate_to_sleep: minimun rate needed to sleep until
         it will be reset
    """
    version = '0.1.0'

    def __init__(self, owner=None, repository=None,
                 api_token=None,
                 base_url=None,
                 tag=None,
                 cache=None,
                 sleep_for_rate=False, min_rate_to_sleep=MIN_RATE_LIMIT):

        origin = base_url if base_url else GITLAB_URL
        origin = urijoin(origin, owner, repository)

        super().__init__(origin, tag=tag, cache=cache)
        self.owner = owner
        self.repository = repository
        self.api_token = api_token
        self.client = GitLabClient(owner, repository, api_token, base_url,
                                   sleep_for_rate, min_rate_to_sleep)
        self._users = {}  # internal users cache

    @classmethod
    def has_caching(cls):
        """Returns whether it supports caching items on the fetch process.

        :returns: this backend supports items cache
        """
        return True

    @classmethod
    def has_resuming(cls):
        """Returns whether it supports to resume the fetch process.

        :returns: this backend supports items resuming
        """
        return True

    @staticmethod
    def metadata_id(item):
        """Extracts the identifier from a GitLab item."""

        return str(item['id'])

    @staticmethod
    def metadata_updated_on(item):
        """Extracts the update time from a GitLab item.

        The timestamp used is extracted from 'updated_at' field.
        This date is converted to UNIX timestamp format. As GitLab
        dates are in UTC the conversion is straightforward.

        :param item: item generated by the backend

        :returns: a UNIX timestamp
        """
        ts = item['updated_at']
        ts = str_to_datetime(ts)

        return ts.timestamp()

    @staticmethod
    def metadata_category(item):
        """Extracts the category from a GitLab item.

        This backend only generates one type of item which is
        'issue'.
        """
        return 'issue'

    @metadata
    def fetch(self, from_date=DEFAULT_DATETIME):
        """Fetch the issues from the repository.

        The method retrieves, from a GitLab repository, the issues
        updated since the given date.

        :param from_date: obtain issues updated since this date

        :returns: a generator of issues
        """

        self._purge_cache_queue()

        from_date = datetime_to_utc(from_date)

        issues_groups = self.client.issues(start=from_date)

        for raw_issues in issues_groups:
            self._push_cache_queue('{ISSUES}')
            self._push_cache_queue(raw_issues)
            self._flush_cache_queue()
            issues = json.loads(raw_issues)
            for issue in issues:
                self.__init_extra_issue_fields(issue)

                #===============================================================
                # if str_to_datetime(issue['updated_at']) < from_date:
                #     continue
                #===============================================================

                issue['user_notes_data'] = \
                    self.__get_issue_notes(issue['iid'])
                issue['award_emoji_data'] = \
                    self.__get_issue_award_emoji(issue['iid'])

                self._push_cache_queue('{ISSUE-END}')
                self._flush_cache_queue()

                yield issue
        self._push_cache_queue('{}{}')
        self._flush_cache_queue()

    @metadata
    def fetch_from_cache(self):
        """Fetch the issues from the cache.
        It returns the issues stored in the cache object provided during
        the initialization of the object. If this method is called but
        no cache object was provided, the method will raise a `CacheError`
        exception.
        :returns: a generator of items
        :raises CacheError: raised when an error occurs accessing the
            cache
        """
        if not self.cache:
            raise CacheError(cause="cache instance was not provided")

        cache_items = self.cache.retrieve()
        raw_item = next(cache_items)

        while raw_item != '{}{}':

            if raw_item == '{ISSUES}':
                issues = self.__fetch_issues_from_cache(cache_items)

            raw_item = next(cache_items)
            for issue in issues:
                self.__init_extra_issue_fields(issue)

                while raw_item != '{ISSUE-END}':
                    try:
                        if raw_item == '{NOTES}':
                            notes = self.__fetch_notes_from_cache(cache_items)
                            issue['user_notes_data'] = notes
                        elif raw_item == '{ISSUE-EMOJIS}':
                            emoji = \
                                self.__fetch_issue_emoji_from_cache(
                                    cache_items)
                            issue['award_emoji_data'] = emoji
                        raw_item = next(cache_items)

                    except StopIteration:
                        # this should be never executed,
                        # the while condition prevents
                        # to trigger the StopIteration exception
                        break

                raw_item = next(cache_items)
                yield issue

    def __get_issue_notes(self, issue_id):
        """Get issue notes"""

        notes = []

        group_notes = self.client.issue_notes(issue_id)
        self._push_cache_queue('{NOTES}')
        self._flush_cache_queue()

        for raw_notes in group_notes:
            self._push_cache_queue(raw_notes)
            self._flush_cache_queue()

            for note in json.loads(raw_notes):
                note_id = note['id']
                note['award_emoji_data'] = \
                    self.__get_note_award_emoji(issue_id, note_id)
                notes.append(note)

        return notes

    def __get_issue_award_emoji(self, issue_id):
        """Get award emojis for issue"""

        emojis = []
        self._push_cache_queue('{ISSUE-EMOJIS}')
        self._flush_cache_queue()

        group_emojis = self.client.issue_emojis(issue_id)
        for raw_emojis in group_emojis:
            self._push_cache_queue(raw_emojis)
            self._flush_cache_queue()

            for emoji in json.loads(raw_emojis):
                emojis.append(emoji)

        return emojis

    def __get_note_award_emoji(self, issue_id, note_id):
        """Fetch emojis for note"""

        emojis = []
        self._push_cache_queue('{NOTE-EMOJIS}')
        self._flush_cache_queue()

        group_emojis = self.client.note_emojis(issue_id, note_id)
        for raw_emojis in group_emojis:
            self._push_cache_queue(raw_emojis)
            self._flush_cache_queue()

            for emoji in json.loads(raw_emojis):
                emojis.append(emoji)

        return emojis

    def __fetch_issues_from_cache(self, cache_items):
        """Fetch issues from cache"""

        raw_content = next(cache_items)
        issues = json.loads(raw_content)
        return issues

    def __fetch_issue_emoji_from_cache(self, cache_items):
        """Fetch issue emoji from cache"""

        emojis = []

        raw_content = next(cache_items)
        group_emojis = json.loads(raw_content)
        for emoji in group_emojis:
            emojis.append(emoji)

        return emojis

    def __fetch_notes_from_cache(self, cache_items):
        """Fetch issue notes from cache"""

        raw_content = next(cache_items)
        notes = json.loads(raw_content)

        for note in notes:
            _ = next(cache_items)
            raw_emoji = next(cache_items)
            note['award_emoji_data'] = json.loads(raw_emoji)

        return notes

    def __init_extra_issue_fields(self, issue):
        """Add fields to an issue"""

        issue['user_notes_data'] = []
        issue['award_emoji_data'] = []


class GitLabClient:
    """Client for retieving information from GitLab API"""

    MAX_RETRIES = 5
    _users = {}       # users cache
    _users_orgs = {}  # users orgs cache

    def __init__(self, owner, repository, token, base_url=None,
                 sleep_for_rate=False, min_rate_to_sleep=MIN_RATE_LIMIT):
        self.owner = owner
        self.repository = repository
        self.token = token
        self.rate_limit = None
        self.sleep_for_rate = sleep_for_rate

        if base_url:
            parts = urllib.parse.urlparse(base_url)
            self.api_url = parts.scheme + '://' + parts.netloc + '/api/v4'
        else:
            self.api_url = GITLAB_API_URL

        if min_rate_to_sleep > MAX_RATE_LIMIT:
            msg = "Minimum rate to sleep value exceeded (%d)."
            msg += "High values might cause the client to sleep forever."
            msg += "Reset to %d."
            logger.warning(msg, min_rate_to_sleep, MAX_RATE_LIMIT)

        self.min_rate_to_sleep = min(min_rate_to_sleep, MAX_RATE_LIMIT)

    def issue_notes(self, issue_id):
        """Get the issue notes from pagination"""

        return self._fetch_items(
            urijoin("issues", str(issue_id), "notes"), "note")

    def issues(self, start=None):
        """Get the issues from pagination"""

        return self._fetch_items("issues", payload_type="issue", start=start)

    def issue_emojis(self, issue_id):
        """Get emojis of an issue"""

        return self._fetch_items(
            urijoin("issues", str(issue_id), "award_emoji"), "emoji")

    def note_emojis(self, issue_id, note_id):
        """Get emojis of a note"""

        return self._fetch_items(
            urijoin("issues", str(issue_id), "notes",
                    str(note_id), "award_emoji"), "emoji")

    def user(self, username):
        """Get the user information and update the user cache"""

        if username in self._users:
            return self._users[username]

        url_user = urijoin(self.api_url, 'users?username=' + username)

        logging.info("Getting info for %s" % (url_user))
        r = self.__send_request(url_user, headers=self.__build_headers())
        user = r.text
        self._users[username] = user

        return user

    def __get_url_repo(self):
        """Build URL repo"""

        url_repo = \
            urijoin(
                self.api_url, 'projects', self.owner + '%2F' + self.repository)

        return url_repo

    def __get_url(self, path, startdate=None):
        """Build repo-"related URL"""

        url = urijoin(self.__get_url_repo(), path)
        return url

    def __build_payload(self, payload_type='issue', startdate=None):
        """Build payload"""

        # Gitlab sort arguments
        payload = {'sort': 'asc'}
        if payload_type == 'issue':
            payload['state'] = 'all'
            payload['order_by'] = 'updated_at'
        elif payload_type in ['note', 'emoji']:
            payload['order_by'] = 'updated_at'

        if startdate:
            startdate = startdate.isoformat()
            payload['created_after'] = startdate

        return payload

    def __build_headers(self):
        """Set header for request"""
        if self.token:
            headers = {'PRIVATE-TOKEN': self.token}

            return headers

    def __send_request(self, url, params=None, headers=None):
        """GET HTTP caring of rate limit"""

        retries = 0

        while retries < self.MAX_RETRIES:
            try:
                if self.rate_limit is not None \
                        and self.rate_limit <= self.min_rate_to_sleep:

                    seconds_to_reset = 3600
                    cause = "GitLab rate limit exhausted."
                    if self.sleep_for_rate:
                        logger.info("%s Waiting %i secs for rate limit reset.",
                                    cause, seconds_to_reset)
                        time.sleep(seconds_to_reset)
                    else:
                        raise RateLimitError(
                            cause=cause, seconds_to_reset=seconds_to_reset)

                r = requests.get(url, params=params, headers=headers)
                r.raise_for_status()

                # GitLab service establishes API rate limits.
                if 'RateLimit-Remaining' in r.headers:
                    self.rate_limit = int(r.headers['RateLimit-Remaining'])
                    logger.debug("Rate limit: %s", self.rate_limit)
                else:
                    self.rate_limit = None

                break
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 403:
                    retry_seconds = int(10)
                    logger.warning(
                        "Abuse rate limit, the backend will sleep for %s seconds \
                            before starting again",
                        retry_seconds)
                    time.sleep(retry_seconds)
                    retries += 1
                else:
                    raise e

        if retries == self.MAX_RETRIES:
            r.raise_for_status()

        return r

    def _fetch_items(self, path, payload_type, start=None):
        """Return the items from gitalb API using links pagination"""

        page = 0  # current page
        last_page = None  # last page
        url_next = self.__get_url(path, start)

        logger.debug("Get GitLab paginated items from " + url_next)
        payload = self.__build_payload(payload_type, start)
        r = self.__send_request(url_next, payload, self.__build_headers())
        items = r.text
        page += 1

        if 'last' in r.links:
            last_url = r.links['last']['url']
            last_page = last_url.split('&page=')[1].split('&')[0]
            last_page = int(last_page)
            logger.debug("Page: %i/%i" % (page, last_page))

        while items:
            yield items

            items = None

            if 'next' in r.links:
                url_next = r.links['next']['url']  # Loving requests :)
                r = self.__send_request(
                    url_next, payload, self.__build_headers())
                page += 1
                items = r.text
                logger.debug("Page: %i/%i" % (page, last_page))


class GitLabCommand(BackendCommand):
    """Class to run GitLab backend from the command line."""

    BACKEND = GitLab

    @staticmethod
    def setup_cmd_parser():
        """Returns the GitLab argument parser."""

        parser = BackendCommandArgumentParser(from_date=True,
                                              token_auth=True,
                                              cache=True)

        # GitLab options
        group = parser.parser.add_argument_group('GitLab arguments')
        group.add_argument('--enterprise-url', dest='base_url',
                           help="Base URL for GitLab Enterprise instance")
        group.add_argument('--sleep-for-rate', dest='sleep_for_rate',
                           action='store_true',
                           help="sleep for getting more rate")
        group.add_argument('--min-rate-to-sleep', dest='min_rate_to_sleep',
                           default=MIN_RATE_LIMIT, type=int,
                           help="sleep until reset when the rate limit \
                               reaches this value")

        # Positional arguments
        parser.parser.add_argument('owner',
                                   help="GitLab owner")
        parser.parser.add_argument('repository',
                                   help="GitLab repository")

        return parser